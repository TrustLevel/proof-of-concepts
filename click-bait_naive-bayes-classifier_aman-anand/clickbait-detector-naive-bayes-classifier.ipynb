{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clickbait detector using Naive Bayes Classifier\n",
    "\n",
    "This kernel focuses on classifying News headlines into clickbaits and non-clickbaits.\n",
    "\n",
    "The clickbaits are labelled as **1** and non-clickbaits as **0**.\n",
    "The headlines are collected from different news sites.\n",
    "\n",
    "The dataset consists of 32000 headlines of which 50% are clickbaits and the other 50% are non-clickbait.\n",
    "\n",
    "I have used a *Multinomial Naive Bayes* classification algorithm for text classification of the given dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing different tools and libraries\n",
    "\n",
    "The main libraries used are *Numpy*, *Pandas*, *NLTK*(Natural language toolkit) and *Scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cc/6b7fky_129vg9v2n5103m8t00000gr/T/ipykernel_66059/4097888231.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string as s\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Should I Get Bings?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  clickbait\n",
       "0                                Should I Get Bings?          1\n",
       "1      Which TV Female Friend Group Do You Belong In          1\n",
       "2  The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
       "3  This Vine Of New York On \"Celebrity Big Brothe...          1\n",
       "4  A Couple Did A Stunning Photo Shoot With Their...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb_data= pd.read_csv('clickbait_data_sample.csv')\n",
    "cb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into Train and Test sets\n",
    "\n",
    "The dataset is splitted into training and testing sets. The percentage of training data is 75% and testing data is 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=cb_data.headline\n",
    "y=cb_data.clickbait\n",
    "train_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.25,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of elements in training set\n",
      "324\n",
      "No. of elements in testing set\n",
      "108\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of elements in training set\")\n",
    "print(train_x.size)\n",
    "print(\"No. of elements in testing set\")\n",
    "print(test_x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     This Country Singer Makes Music On His Game Bo...\n",
       "394            Seniors Help Michigan St. Maintain Course\n",
       "358                                 Schedule, Day by Day\n",
       "414                     Caged children well fed, behaved\n",
       "298                   Panda gives birth at San Diego Zoo\n",
       "230                  Alan Turing given posthumous pardon\n",
       "425                   The Charm and Silliness of Round 1\n",
       "285    Thousands evacuated after chemical truck overt...\n",
       "65     29 Books Every '90s Kid Will Immediately Recog...\n",
       "175    23 Important Life Lessons \"The Hunger Games\" H...\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     1\n",
       "394    0\n",
       "358    0\n",
       "414    0\n",
       "298    0\n",
       "230    0\n",
       "425    0\n",
       "285    0\n",
       "65     1\n",
       "175    1\n",
       "Name: clickbait, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20                 What New Thing Should You Try In 2016\n",
       "386      Obama Seeks Action Against Credit Card Industry\n",
       "178    17 Things Only Women Who Aren't That Into Make...\n",
       "198    You'll Be In Tears After Hearing Josh Groban A...\n",
       "89      42 Of The Most Romantic Lines From YA Literature\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20     1\n",
       "386    0\n",
       "178    1\n",
       "198    1\n",
       "89     1\n",
       "Name: clickbait, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization of Data\n",
    "\n",
    "The data is tokenized i.e. split into tokens which are the smallest or minimal meaningful units. The data is split into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    lst=text.split()\n",
    "    return lst\n",
    "train_x=train_x.apply(tokenization)\n",
    "test_x=test_x.apply(tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to lowercase\n",
    "\n",
    "The data is converted into lowercase to avoid ambiguity between same words in different cases like 'NLP', 'nlp' or 'Nlp'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        i=i.lower()\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(lowercasing)\n",
    "test_x=test_x.apply(lowercasing)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing punctuation\n",
    "\n",
    "The punctuations are removed to increase the efficiency of the model. They are irrelevant because they provide no added information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        for j in s.punctuation:\n",
    "            i=i.replace(j,'')\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(remove_punctuations)\n",
    "test_x=test_x.apply(remove_punctuations)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(lst):\n",
    "    nodig_lst=[]\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        for j in s.digits:    \n",
    "            i=i.replace(j,'')\n",
    "        nodig_lst.append(i)\n",
    "    for i in nodig_lst:\n",
    "        if i!='':\n",
    "            new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(remove_numbers)\n",
    "test_x=test_x.apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All stopwords of English language \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"All stopwords of English language \")\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(lst):\n",
    "    stop=stopwords.words('english')\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        if i not in stop:\n",
    "            new_lst.append(i)\n",
    "    return new_lst\n",
    "\n",
    "train_x=train_x.apply(remove_stopwords)\n",
    "test_x=test_x.apply(remove_stopwords)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        i=i.strip()\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(remove_spaces)\n",
    "test_x=test_x.apply(remove_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing data after preprocessing\n",
    "\n",
    "After preprocessing the data i.e. after removing punctuation, stopwords, spaces and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29     [country, singer, makes, music, game, boy, spa...\n",
       "394      [seniors, help, michigan, st, maintain, course]\n",
       "358                                 [schedule, day, day]\n",
       "414                [caged, children, well, fed, behaved]\n",
       "298               [panda, gives, birth, san, diego, zoo]\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20                                     [new, thing, try]\n",
       "386       [obama, seeks, action, credit, card, industry]\n",
       "178           [things, women, arent, makeup, understand]\n",
       "198    [youll, tears, hearing, josh, groban, kelly, c...\n",
       "89                     [romantic, lines, ya, literature]\n",
       "Name: headline, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. It involves the morphological analysis of words.\n",
    "\n",
    "In lemmatization we find the root word or base form of the word rather than just clipping some characters from the end e.g. *is, are, am* are all converted to its base form *be* in Lemmatization\n",
    "\n",
    "Here lemmatization is done using NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/vw9yis9/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "lemmatizer=nltk.stem.WordNetLemmatizer()\n",
    "def lemmatzation(lst):\n",
    "    new_lst=[]\n",
    "    for i in lst:\n",
    "        i=lemmatizer.lemmatize(i)\n",
    "        new_lst.append(i)\n",
    "    return new_lst\n",
    "train_x=train_x.apply(lemmatzation)\n",
    "test_x=test_x.apply(lemmatzation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\n",
    "test_x=test_x.apply(lambda x: ''.join(i+' ' for i in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': 28,\n",
       " 'o': 37,\n",
       " 'u': 25,\n",
       " 'n': 57,\n",
       " 't': 43,\n",
       " 'r': 49,\n",
       " 'y': 18,\n",
       " 's': 34,\n",
       " 'i': 55,\n",
       " 'g': 20,\n",
       " 'e': 89,\n",
       " 'm': 24,\n",
       " 'a': 54,\n",
       " 'k': 8,\n",
       " 'b': 12,\n",
       " 'p': 23,\n",
       " 'h': 20,\n",
       " 'l': 33,\n",
       " 'd': 31,\n",
       " 'w': 8,\n",
       " 'f': 7,\n",
       " 'v': 10,\n",
       " 'z': 2,\n",
       " 'j': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dist={}\n",
    "for i in train_x.head(20):\n",
    "    x=i.split()\n",
    "    for j in x:\n",
    "        if j not in freq_dist.keys():\n",
    "            freq_dist[j]=1\n",
    "        else:\n",
    "            freq_dist[j]+=1\n",
    "freq_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (Term frequency-Inverse Data Frequency)\n",
    "\n",
    "This method is used to convert the text into features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[1;32m      2\u001b[0m tfidf\u001b[38;5;241m=\u001b[39mTfidfVectorizer()\n\u001b[0;32m----> 3\u001b[0m train_1\u001b[38;5;241m=\u001b[39m\u001b[43mtfidf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_1\u001b[38;5;241m=\u001b[39mtfidf\u001b[38;5;241m.\u001b[39mtransform(test_x)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/click-bait_naive-bayes-classifier_aman-ana-qBVHTcz5/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2137\u001b[0m )\n\u001b[0;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/click-bait_naive-bayes-classifier_aman-ana-qBVHTcz5/lib/python3.10/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/click-bait_naive-bayes-classifier_aman-ana-qBVHTcz5/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/click-bait_naive-bayes-classifier_aman-ana-qBVHTcz5/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1297\u001b[0m         )\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer()\n",
    "train_1=tfidf.fit_transform(train_x)\n",
    "test_1=tfidf.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features extracted\n",
      "18248\n",
      "\n",
      "The 100 features extracted from TF-IDF \n",
      "['aa', 'aaevpc', 'aaron', 'ab', 'abandon', 'abandoned', 'abandoning', 'abba', 'abbas', 'abbey', 'abbott', 'abby', 'abc', 'abdallahi', 'abdelbaset', 'abdicates', 'abduct', 'abducted', 'abduction', 'abductor', 'abdul', 'abdullah', 'abdulmutallab', 'abel', 'abercrombie', 'abhishek', 'abhisit', 'abide', 'ability', 'abin', 'abitibibowater', 'abject', 'abkhazia', 'ablaze', 'able', 'aboard', 'abolish', 'abombs', 'aborigine', 'aborted', 'abortion', 'abortionrights', 'aboulhosn', 'abound', 'abraham', 'abramoff', 'abrams', 'abroad', 'abrogates', 'abse', 'absence', 'absentee', 'absolute', 'absolutely', 'absolutereturn', 'absorbs', 'abstention', 'abstinence', 'absurd', 'absurdity', 'absurdly', 'abu', 'abuela', 'abuelita', 'abundant', 'abuse', 'abused', 'abuserelated', 'abusing', 'abusive', 'ac', 'academia', 'academic', 'academy', 'acapella', 'acc', 'accelerates', 'accelerating', 'accelerator', 'accent', 'accenture', 'accept', 'acceptance', 'accepted', 'accepting', 'accepts', 'access', 'accessible', 'accession', 'accessory', 'accident', 'accidental', 'accidentally', 'accolade', 'accord', 'according', 'account', 'accountability', 'accountant', 'accounted']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features extracted\")\n",
    "print(len(tfidf.get_feature_names()))\n",
    "print()\n",
    "print(\"The 100 features extracted from TF-IDF \")\n",
    "print(tfidf.get_feature_names()[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set (24000, 18248)\n",
      "Shape of test set (8000, 18248)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train set\",train_1.shape)\n",
    "print(\"Shape of test set\",test_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr=train_1.toarray()\n",
    "test_arr=test_1.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_MN=MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 actual labels:  [0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n",
      "first 20 predicted labels:  [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "NB_MN.fit(train_arr,train_y)\n",
    "pred=NB_MN.predict(test_arr)\n",
    "print('first 20 actual labels: ',test_y.tolist()[:20])\n",
    "print('first 20 predicted labels: ',pred.tolist()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Result\n",
    "\n",
    "The Accuracy and F1 score of the model are printed to evaluate the model for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the model\n",
      "0.9632343959936486\n",
      "Accuracy of the model\n",
      "0.962375\n",
      "Accuracy of the model in percentage\n",
      "96.2375 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "print(\"F1 score of the model\")\n",
    "print(f1_score(test_y,pred))\n",
    "print(\"Accuracy of the model\")\n",
    "print(accuracy_score(test_y,pred))\n",
    "print(\"Accuracy of the model in percentage\")\n",
    "print(accuracy_score(test_y,pred)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[3756  169]\n",
      " [ 132 3943]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      3925\n",
      "           1       0.96      0.97      0.96      4075\n",
      "\n",
      "    accuracy                           0.96      8000\n",
      "   macro avg       0.96      0.96      0.96      8000\n",
      "weighted avg       0.96      0.96      0.96      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_y,pred))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_y,pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A second model for TF-IDF with different n-grams and fixed feature size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model in percentage\n",
      "95.7125 %\n",
      "Confusion Matrix\n",
      "[[3774  151]\n",
      " [ 192 3883]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96      3925\n",
      "           1       0.96      0.95      0.96      4075\n",
      "\n",
      "    accuracy                           0.96      8000\n",
      "   macro avg       0.96      0.96      0.96      8000\n",
      "weighted avg       0.96      0.96      0.96      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(ngram_range=(1,3),max_features=6500)\n",
    "train_2=tfidf.fit_transform(train_x)\n",
    "test_2=tfidf.transform(test_x)\n",
    "\n",
    "NB_MN.fit(train_2.toarray(),train_y)\n",
    "pred2=NB_MN.predict(test_2.toarray())\n",
    "\n",
    "print(\"Accuracy of the model in percentage\")\n",
    "print(accuracy_score(test_y,pred2)*100,\"%\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(test_y,pred2))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(test_y,pred2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "click-bait_naive-bayes-classifier_aman-ana-qBVHTcz5",
   "language": "python",
   "name": "click-bait_naive-bayes-classifier_aman-ana-qbvhtcz5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
